{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from elephas.spark_model import SparkModel\n",
    "from create_and_train_biLSTM_Youtube import create_model\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Youtube-8M') \\\n",
    "                  .set(\"spark.jars\",\n",
    "                       \"ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dir = \"yt8pm_100th_shard/v2\"\n",
    "data_path = lambda level, set_name: \"{}/{}/{}*.tfrecord\".format(top_dir, level, set_name)\n",
    "\n",
    "# just work with ONE TF record for now.\n",
    "\n",
    "# VIDEO-LEVEL\n",
    "#vid_train_df = spark.read.format(\"tfrecords\").option(\"recordType\", \"Example\").load(data_path('video','train'))\n",
    "\n",
    "vid_train_df = spark.read.format(\"tfrecords\").option(\"recordType\", \"Example\").load(\"mys3bucket/yt8pm_100th_shard/v2/video/train0093.tfrecord\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "    \n",
    "fields = [StructField(\"id\", StringType()),\n",
    "          StructField(\"labels\", ArrayType(IntegerType(), True)),\n",
    "          StructField(\"rgb\", ArrayType(ArrayType(StringType(), True),True)),\n",
    "          StructField(\"audio\", ArrayType(ArrayType(StringType(), True),True))]\n",
    "                      \n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(labels,ArrayType(IntegerType,true),true),StructField(rgb,ArrayType(ArrayType(StringType,true),true),true),StructField(audio,ArrayType(ArrayType(StringType,true),true),true)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FRAME-LEVEL\n",
    "#frame_train_df = spark.read.format(\"tfrecords\").option(\"recordType\", \"SequenceExample\").load(data_path('frame','train'))\n",
    "sess = tf.InteractiveSession()\n",
    "frame_train_df = spark.read.format(\"tfrecords\").schema(schema).option(\"recordType\", \"SequenceExample\").load(\"mys3bucket/yt8pm_100th_shard/v2/frame/train0093.tfrecord\")\n",
    "tf.reset_default_graph()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = frame_train_df.select('audio').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d98a60f61856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-d98a60f61856>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "[ a.decode(\"utf-8\") for a in audio[0][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----+\n",
      "|          mean_audio|              labels|            mean_rgb|  id|\n",
      "+--------------------+--------------------+--------------------+----+\n",
      "|[-1.2556146, 0.17...|             [0, 12]|[0.5198898, 0.301...|eXbF|\n",
      "|[-0.32460678, -0....|[16, 25, 189, 645...|[-0.5026991, -1.6...|BFbF|\n",
      "|[-1.7352352, 1.83...|[2, 44, 64, 113, ...|[0.24258906, 0.97...|GqbF|\n",
      "|[0.7349236, 1.268...|                 [3]|[-0.026906455, -0...|XabF|\n",
      "|[1.2375641, -0.14...|              [1, 5]|[-0.45482802, -1....|3mbF|\n",
      "|[0.50689745, 0.02...|                [14]|[0.45552492, 0.64...|S6bF|\n",
      "|[0.71223485, 1.41...|      [3, 4, 13, 54]|[-0.022711225, 0....|mXbF|\n",
      "|[-0.662256, -0.97...|           [11, 579]|[0.3059462, 0.947...|7sbF|\n",
      "|[0.9852263, 0.145...|[2, 76, 227, 474,...|[0.24360302, 0.40...|H1bF|\n",
      "|[0.10193015, -0.9...| [49, 80, 265, 2063]|[0.60755104, 0.43...|fxbF|\n",
      "|[0.2003511, -1.10...|     [0, 1, 36, 132]|[-0.4298068, -1.0...|w1bF|\n",
      "|[-0.5392725, -0.9...|       [39, 50, 503]|[0.17366871, 0.95...|3obF|\n",
      "|[-0.6388898, 1.07...|           [3, 2195]|[0.089333594, 0.0...|LObF|\n",
      "|[-1.1066483, -0.6...|             [0, 12]|[0.7414522, -1.01...|YwbF|\n",
      "|[1.0145974, 0.175...|      [61, 227, 474]|[-0.25524384, 0.0...|DxbF|\n",
      "|[1.333466, -0.435...|           [5, 3226]|[-0.1797692, -1.2...|NYbF|\n",
      "|[-0.9178203, -0.7...|   [21, 23, 24, 758]|[-0.19473058, 0.6...|I6bF|\n",
      "|[-1.2025927, -0.8...|      [39, 156, 202]|[0.68875366, 0.31...|jmbF|\n",
      "|[0.8315929, -0.57...|       [5, 644, 769]|[-0.0593875, -0.7...|FHbF|\n",
      "|[-1.7979561, -0.9...|[21, 23, 24, 106,...|[-0.9477386, 0.26...|PIbF|\n",
      "+--------------------+--------------------+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vid_train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+\n",
      "|  id|              labels|                 rgb|               audio|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "|lKbF| [1, 19, 2852, 3477]|[WrappedArray(i�^...|[WrappedArray(�C�...|\n",
      "|HebF|              [3, 6]|[WrappedArray(��...|[WrappedArray(���...|\n",
      "|GkbF|                [66]|[WrappedArray(��m...|[WrappedArray(�K�...|\n",
      "|q2bF|                [14]|[WrappedArray(\u0000H�...|[WrappedArray(>�\u0000...|\n",
      "|aUbF|     [141, 247, 255]|[WrappedArray(!bb...|[WrappedArray(�\u001b~...|\n",
      "|A9bF|[2, 17, 30, 55, 174]|[WrappedArray(��B...|[WrappedArray(�\u001b~...|\n",
      "|52bF|                [25]|[WrappedArray(\u0000��...|[WrappedArray(�]�...|\n",
      "|gNbF|              [2, 7]|[WrappedArray(\u0000H�...|[WrappedArray(�+s...|\n",
      "|6ZbF|       [1, 246, 658]|[WrappedArray(5.[...|[WrappedArray(e��...|\n",
      "|rubF|            [39, 66]|[WrappedArray(a�u...|[WrappedArray(g<�...|\n",
      "|UqbF|                [11]|[WrappedArray(\u0000z/...|[WrappedArray(�\u001b~...|\n",
      "|ATbF|   [0, 1, 245, 2799]|[WrappedArray(\u0013�Z...|[WrappedArray(�;�...|\n",
      "|3lbF|[39, 181, 216, 22...|[WrappedArray(h�P...|[WrappedArray(�\u001b~...|\n",
      "|UGbF|   [3, 4, 6, 10, 13]|[WrappedArray(�\u0012�...|[WrappedArray(Č�e...|\n",
      "|oJbF|   [0, 1, 337, 1116]|[WrappedArray(\u0000H�...|[WrappedArray(�\u001b~...|\n",
      "|exbF|                [12]|[WrappedArray(�\u0017�...|[WrappedArray(���...|\n",
      "|CFbF|[39, 121, 156, 39...|[WrappedArray(\u0000H�...|[WrappedArray(�Oq...|\n",
      "|CsbF|[3, 4, 6, 9, 10, ...|[WrappedArray(D��...|[WrappedArray(�]~...|\n",
      "|pUbF|              [0, 1]|[WrappedArray(\u0000H�...|[WrappedArray(�4p...|\n",
      "|DLbF|               [333]|[WrappedArray(\u0000^�...|[WrappedArray(�?^...|\n",
      "+----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to convert wrapped array???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickly check we have agreement on the number of videos in video-level and feature-level IDs\n",
    "frame_ids = frame_train_df.select('id')\n",
    "video_ids = vid_train_df.select('id')\n",
    "video_ids = video_ids.toPandas()\n",
    "frame_ids = frame_ids.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1015, 1), (1015, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_ids.shape,frame_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = video_ids.id.tolist()\n",
    "frame_ids = frame_ids.id.tolist()\n",
    "assert len(set(video_ids).intersection(set(frame_ids))) == frame_train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, as seen above, the TF records contain same videos on video and feature level for each TF Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop labels from one dataframe - its duplicate!\n",
    "frame_train_df = frame_train_df.drop('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILIP NOTE: will this traditional SQL-like join work in memory on the whole thing?\n",
    "# we have two options here: either perform all the operations with SQL or in RDD\n",
    "# we will stick to SQL for now since IT IS MUCH EASIER.\n",
    "\n",
    "total_spark_df = vid_train_df.join(frame_train_df,on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  id|          mean_audio|              labels|            mean_rgb|               audio|                 rgb|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|3cbF|[1.0291835, -0.32...|       [10, 97, 243]|[-0.1551302, 0.97...|[WrappedArray(�\u001b~...|[WrappedArray(\u0000H�...|\n",
      "|KtbF|[1.175444, 1.8621...|       [3, 4, 6, 38]|[-0.8292868, 0.38...|[WrappedArray(���...|[WrappedArray(\u0019��...|\n",
      "|TjbF|[1.0009577, -0.45...|[2, 17, 76, 1068,...|[-0.59980744, 0.0...|[WrappedArray(�m�...|[WrappedArray(\\�<...|\n",
      "|UXbF|[-1.1192986, -0.5...|[15, 60, 239, 363...|[0.38621774, 0.99...|[WrappedArray(�\u001b~...|[WrappedArray(\u0000H�...|\n",
      "|bobF|[0.22682534, 1.62...| [82, 103, 118, 329]|[0.5607807, 0.684...|[WrappedArray(��֬...|[WrappedArray(�q�...|\n",
      "|nKbF|[-0.025651552, -0...|[11, 20, 22, 29, ...|[0.012622957, 1.0...|[WrappedArray(qXݗ...|[WrappedArray(\u0000R�...|\n",
      "|sCbF|[1.1240029, 0.165...|              [0, 1]|[0.9078685, -0.99...|[WrappedArray(�HQ...|[WrappedArray(\u0000H�...|\n",
      "|OcbF|[0.17827001, 0.03...|          [269, 320]|[-0.36897182, -0....|[WrappedArray(�7�...|[WrappedArray(�se...|\n",
      "|g8bF|[-0.5280215, -1.0...|              [0, 1]|[1.1547292, -0.74...|[WrappedArray(�;h...|[WrappedArray(\u0000H�...|\n",
      "|zubF|[-0.710828, -0.56...|  [11, 20, 22, 1049]|[1.4221916, 1.174...|[WrappedArray(�~�...|[WrappedArray(M�U...|\n",
      "|1DbF|[0.92227, 0.09858...|       [11, 83, 720]|[-0.3453901, 0.18...|[WrappedArray(\u0000�\u0000...|[WrappedArray(^�T...|\n",
      "|6tbF|[-1.7041802, 0.65...|[2, 75, 95, 165, ...|[-0.43851674, 1.2...|[WrappedArray(\u0000�h...|[WrappedArray()�c...|\n",
      "|I0bF|[-0.9086527, -1.0...|              [1076]|[1.201581, 0.5146...|[WrappedArray(q!�...|[WrappedArray(ʛ[/...|\n",
      "|rqbF|[1.2381916, -0.34...|  [5, 16, 516, 3638]|[-0.5074293, -0.4...|[WrappedArray(�>z...|[WrappedArray(\u0000�B...|\n",
      "|BxbF|[-1.4123706, 1.52...|             [0, 12]|[-0.21608293, 0.0...|[WrappedArray()�I...|[WrappedArray(�~�...|\n",
      "|ambF|[-0.5373061, -0.5...|              [2, 7]|[-0.13586885, 0.6...|[WrappedArray(qGh...|[WrappedArray(\u0000H�...|\n",
      "|eXbF|[-1.2556146, 0.17...|             [0, 12]|[0.5198898, 0.301...|[WrappedArray(�A...|[WrappedArray(\u0000H�...|\n",
      "|xFbF|[0.9054952, -0.71...|              [6, 8]|[-0.43044218, 0.2...|[WrappedArray(dK�...|[WrappedArray(\u0000H�...|\n",
      "|8qbF|[-0.48818308, -0....|                [25]|[-0.20428361, 0.4...|[WrappedArray(�j�...|[WrappedArray(\u0000 o...|\n",
      "|BlbF|[1.1781437, -0.09...|       [53, 58, 228]|[-0.009371902, -0...|[WrappedArray(���...|[WrappedArray(\u00002`...|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = total_spark_df.select('mean_audio','mean_rgb','audio','rgb','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean_audio', 'mean_rgb', 'audio', 'rgb', 'labels']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = total_spark_df.select('audio').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1015, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['�\\x1b~_�YC����Dq�p\\x19d���\\x00�^�P�q��.�vpF����϶m�D�~��f�GzO�=P�M�g�h��g�yc�t�|imhx~��Z~��l�qyqvlu��p�y���m��t��~�ilp��z��syf}ruy~~�r���']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_y(raw_labels,label_size=20):\n",
    "#     '''\n",
    "#     Helper function to transform labels into one-hot TOP 20\n",
    "#     Uses np.unique(return_counts=True) as implicit sorter (first K labels are the most frequent)\n",
    "#     '''\n",
    "#     all_labels = []\n",
    "#     for i in list(raw_labels):\n",
    "#         for j in list(i):\n",
    "#             all_labels.append(j)\n",
    "\n",
    "#     results = np.unique(all_labels,return_counts=True)\n",
    "#     labels_vocab,counts = results\n",
    "\n",
    "#     labels = labels_vocab[:label_size-1] #last columns will be 1 if none of those labels found in a video\n",
    "#     output = []\n",
    "#     for set_of_labels in raw_labels:\n",
    "        \n",
    "#         # preallocate numpy arr for each set of labels\n",
    "#         sequence = np.zeros(label_size)\n",
    "#         # loop through all the labels in one video and flip them to 1s\n",
    "#         for this_label in set_of_labels:\n",
    "#             designation = np.where(labels==this_label)\n",
    "#             for des in designation:\n",
    "#                 sequence[des]=1\n",
    "#         # done with one training points\n",
    "#         if sequence.sum()==0:\n",
    "#             sequence[-1]=1\n",
    "#         output.append(sequence)\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf_one_hot_y = udf(one_hot_y, ArrayType(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.withColumn('labels',udf_one_hot_y(train_df['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df = train_df.select('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_rdd = labels_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          mean_audio|            mean_rgb|               audio|                 rgb|              labels|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[1.0291835, -0.32...|[-0.1551302, 0.97...|[WrappedArray(�\u001b~...|[WrappedArray(\u0000H�...|       [10, 97, 243]|\n",
      "|[1.175444, 1.8621...|[-0.8292868, 0.38...|[WrappedArray(���...|[WrappedArray(\u0019��...|       [3, 4, 6, 38]|\n",
      "|[1.0009577, -0.45...|[-0.59980744, 0.0...|[WrappedArray(�m�...|[WrappedArray(\\�<...|[2, 17, 76, 1068,...|\n",
      "|[-1.1192986, -0.5...|[0.38621774, 0.99...|[WrappedArray(�\u001b~...|[WrappedArray(\u0000H�...|[15, 60, 239, 363...|\n",
      "|[0.22682534, 1.62...|[0.5607807, 0.684...|[WrappedArray(��֬...|[WrappedArray(�q�...| [82, 103, 118, 329]|\n",
      "|[-0.025651552, -0...|[0.012622957, 1.0...|[WrappedArray(qXݗ...|[WrappedArray(\u0000R�...|[11, 20, 22, 29, ...|\n",
      "|[1.1240029, 0.165...|[0.9078685, -0.99...|[WrappedArray(�HQ...|[WrappedArray(\u0000H�...|              [0, 1]|\n",
      "|[0.17827001, 0.03...|[-0.36897182, -0....|[WrappedArray(�7�...|[WrappedArray(�se...|          [269, 320]|\n",
      "|[-0.5280215, -1.0...|[1.1547292, -0.74...|[WrappedArray(�;h...|[WrappedArray(\u0000H�...|              [0, 1]|\n",
      "|[-0.710828, -0.56...|[1.4221916, 1.174...|[WrappedArray(�~�...|[WrappedArray(M�U...|  [11, 20, 22, 1049]|\n",
      "|[0.92227, 0.09858...|[-0.3453901, 0.18...|[WrappedArray(\u0000�\u0000...|[WrappedArray(^�T...|       [11, 83, 720]|\n",
      "|[-1.7041802, 0.65...|[-0.43851674, 1.2...|[WrappedArray(\u0000�h...|[WrappedArray()�c...|[2, 75, 95, 165, ...|\n",
      "|[-0.9086527, -1.0...|[1.201581, 0.5146...|[WrappedArray(q!�...|[WrappedArray(ʛ[/...|              [1076]|\n",
      "|[1.2381916, -0.34...|[-0.5074293, -0.4...|[WrappedArray(�>z...|[WrappedArray(\u0000�B...|  [5, 16, 516, 3638]|\n",
      "|[-1.4123706, 1.52...|[-0.21608293, 0.0...|[WrappedArray()�I...|[WrappedArray(�~�...|             [0, 12]|\n",
      "|[-0.5373061, -0.5...|[-0.13586885, 0.6...|[WrappedArray(qGh...|[WrappedArray(\u0000H�...|              [2, 7]|\n",
      "|[-1.2556146, 0.17...|[0.5198898, 0.301...|[WrappedArray(�A...|[WrappedArray(\u0000H�...|             [0, 12]|\n",
      "|[0.9054952, -0.71...|[-0.43044218, 0.2...|[WrappedArray(dK�...|[WrappedArray(\u0000H�...|              [6, 8]|\n",
      "|[-0.48818308, -0....|[-0.20428361, 0.4...|[WrappedArray(�j�...|[WrappedArray(\u0000 o...|                [25]|\n",
      "|[1.1781437, -0.09...|[-0.009371902, -0...|[WrappedArray(���...|[WrappedArray(\u00002`...|       [53, 58, 228]|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels(labels):\n",
    "    allowed=np.arange(20)\n",
    "    one_hot=np.zeros(20)\n",
    "    for l in labels:\n",
    "        if l in allowed:\n",
    "            one_hot[l]=1\n",
    "    return one_hot\n",
    "\n",
    "# to-do: create a list of top 1000 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_frame(video_frames,max_frame_sequence_length=10):    \n",
    "    return np.vstack(frame)[:max_frame_sequence_length,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order needed: video rgb, video, audio\n",
    "train_rdd_sample = train_rdd.map(lambda x: (np.array(x[1]),np.array(x[0]),x[2],convert_labels(x[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_rwo = train_rdd_sample.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(one_rwo[0][2][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM video classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import operator\n",
    "import time \n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating training and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Model parameters and creating architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10 class problem for now?\n",
    "label_feature_size = 10\n",
    "\n",
    "# how many frames we will use from each video?\n",
    "max_frame_rgb_sequence_length = 10\n",
    "frame_rgb_embedding_size = 1024\n",
    "\n",
    "# how many audio sequences we will use from each video?\n",
    "max_frame_audio_sequence_length = 10\n",
    "frame_audio_embedding_size = 128\n",
    "\n",
    "number_dense_units = 1000\n",
    "number_lstm_units = 100\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "activation_function='relu'\n",
    "validation_split_ratio = 0.2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n",
    "    \n",
    "    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n",
    "    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    \n",
    "    # creating input layer for frame-level data\n",
    "    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n",
    "    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n",
    "    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n",
    "    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n",
    "    \n",
    "    #creating input layer for video-level data \n",
    "    vid_shape=(1024,)\n",
    "    video_rgb_input = Input(shape=vid_shape)\n",
    "    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function, input_shape=vid_shape)(video_rgb_input)\n",
    "    \n",
    "    aud_shape=(128,)\n",
    "    video_audio_input = Input(shape=aud_shape)\n",
    "    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function,input_shape = aud_shape)(video_audio_input)\n",
    "    \n",
    "    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n",
    "    merged_frame = concatenate([frame_x1, frame_x2])\n",
    "    merged_frame = BatchNormalization()(merged_frame)\n",
    "    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n",
    "    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n",
    "    \n",
    "    # merging video-level dense layer output\n",
    "    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n",
    "    merged_video = BatchNormalization()(video_rgb_dense)\n",
    "    merged_video = Dropout(rate_drop_dense)(merged_video)\n",
    "    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n",
    "\n",
    "    \n",
    "    # merging frame-level and video-level dense layer output\n",
    "    merged = concatenate([merged_frame_dense, merged_video_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "     \n",
    "    merged = Dense(number_dense_units, activation=activation_function)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          900000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          183200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          512500      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400)          1600        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500)          2000        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          200500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 500)          250500      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000)         0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1000)         4000        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1000)         1001000     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000)         4000        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           10010       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,069,310\n",
      "Trainable params: 3,063,510\n",
      "Non-trainable params: 5,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41393, 41393, 41393, 41393)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vid_ids), len(train_labels), len(train_mean_rgb), len(train_mean_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only grabbed 5 videos, it took 6 minutes to load!!\n",
    "len(train_frame_ids),len(train_frame_labels),len(train_frame_rgb),len(train_frame_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATbF',\n",
       " 'q2bF',\n",
       " 'exbF',\n",
       " 'gNbF',\n",
       " 'GkbF',\n",
       " 'aUbF',\n",
       " 'A9bF',\n",
       " 'UGbF',\n",
       " 'pUbF',\n",
       " 'DLbF',\n",
       " '52bF',\n",
       " 'CsbF',\n",
       " '3lbF',\n",
       " '6ZbF',\n",
       " 'rubF',\n",
       " 'UqbF',\n",
       " 'HebF',\n",
       " 'oJbF',\n",
       " 'lKbF',\n",
       " 'CFbF']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_frame_ids).intersection(set(train_vid_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATbF',\n",
       " 'q2bF',\n",
       " 'exbF',\n",
       " 'gNbF',\n",
       " 'GkbF',\n",
       " 'aUbF',\n",
       " 'A9bF',\n",
       " 'UGbF',\n",
       " 'pUbF',\n",
       " 'DLbF',\n",
       " '52bF',\n",
       " 'CsbF',\n",
       " '3lbF',\n",
       " '6ZbF',\n",
       " 'rubF',\n",
       " 'UqbF',\n",
       " 'HebF',\n",
       " 'oJbF',\n",
       " 'lKbF',\n",
       " 'CFbF']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_frame_ids).intersection(set(train_vid_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_dataset(video_rgb, video_audio, vid_ids, frame_rgb, frame_audio, frame_labels, frame_ids):\n",
    "    \"\"\"\n",
    "    Method to created training and validation data. \n",
    "    We need to make sure we only use video IDs for which we have frames.\n",
    "    This is handled below.\n",
    "    \n",
    "    \"\"\"\n",
    "    # we have to have the same video of for both video and frame-level features\n",
    "    video_rgb_matching = []\n",
    "    video_audio_matching = []\n",
    "    \n",
    "    for idx in frame_ids: # for each ID available on frame level, find matching video-level features\n",
    "        for i, idx_vid in enumerate(vid_ids): # scan through video-level ids\n",
    "            if idx == idx_vid: \n",
    "                video_rgb_matching.append(video_rgb[i])\n",
    "                video_audio_matching.append(video_audio[i])\n",
    "                \n",
    "                \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(frame_labels)))\n",
    "        \n",
    "    video_rgb_shuffled = np.array(video_rgb_matching)[shuffle_indices]\n",
    "    video_audio_shuffled = np.array(video_audio_matching)[shuffle_indices]\n",
    "    frame_rgb_shuffled = np.array(frame_rgb)[shuffle_indices]\n",
    "    frame_audio_shuffled = np.array(frame_audio)[shuffle_indices]\n",
    "    labels_shuffled = np.array(frame_labels)[shuffle_indices]\n",
    "\n",
    "    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n",
    "    \n",
    "    # delete orig vars to clear some cache\n",
    "    del video_rgb\n",
    "    del video_audio\n",
    "    del frame_rgb\n",
    "    del frame_audio\n",
    "    gc.collect()\n",
    "    \n",
    "    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n",
    "    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n",
    "    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n",
    "    \n",
    "    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n",
    "    gc.collect()\n",
    "    \n",
    "    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, \n",
    "            val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, \\\n",
    "train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels \\\n",
    "                = create_train_dev_dataset(train_mean_rgb, train_mean_audio, train_vid_ids, train_frame_rgb, \\\n",
    "                                            train_frame_audio, train_frame_labels, train_frame_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into final input in the model\n",
    "def one_hot_y(raw_labels=train_labels,label_size=20):\n",
    "    '''\n",
    "    Helper function to transform labels into one-hot TOP 20\n",
    "    Uses np.unique(return_counts=True) as implicit sorter (first K labels are the most frequent)\n",
    "    '''\n",
    "    all_labels = []\n",
    "    for i in list(raw_labels):\n",
    "        for j in list(i):\n",
    "            all_labels.append(j)\n",
    "\n",
    "    results = np.unique(all_labels,return_counts=True)\n",
    "    labels_vocab,counts = results\n",
    "\n",
    "    labels = labels_vocab[:label_size-1] #last columns will be 1 if none of those labels found in a video\n",
    "    output = []\n",
    "    for set_of_labels in raw_labels:\n",
    "        \n",
    "        # preallocate numpy arr for each set of labels\n",
    "        sequence = np.zeros(label_size)\n",
    "        # loop through all the labels in one video and flip them to 1s\n",
    "        for this_label in set_of_labels:\n",
    "            designation = np.where(labels==this_label)\n",
    "            for des in designation:\n",
    "                sequence[des]=1\n",
    "        # done with one training points\n",
    "        if sequence.sum()==0:\n",
    "            sequence[-1]=1\n",
    "        output.append(sequence)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_lstm(video_rgb,video_audio, frame_rgb, frame_audio,\n",
    "                            labels,label_feature_size=10,max_frame_rgb_sequence_length = 10,\\\n",
    "                            max_frame_audio_sequence_length = 10):\n",
    "    frames = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_rgb_sequence_length x 1024)\n",
    "    #print(len(frame_rgb))\n",
    "    \n",
    "    for frame in frame_rgb: \n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames.append(np.vstack(frame)[:max_frame_rgb_sequence_length,:])\n",
    "    #print(len(frames))\n",
    "\n",
    "    frames = np.reshape(np.array(frames),(len(frame_rgb),max_frame_rgb_sequence_length,1024))\n",
    "\n",
    "    #print(frames.shape)\n",
    "    \n",
    "    frames_audio = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_audio_sequence_length x 128)\n",
    "    for frame in frame_audio:\n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames_audio.append(np.vstack(frame)[:max_frame_audio_sequence_length,:])\n",
    "\n",
    "    frames_audio = np.reshape(np.array(frames_audio),(len(frame_audio),max_frame_audio_sequence_length,128))\n",
    "    #print(frames_audio.shape)\n",
    "    \n",
    "    # deal with videos\n",
    "    \n",
    "    video_rgb = np.vstack(video_rgb)\n",
    "    video_audio = np.vstack(video_audio)\n",
    "    \n",
    "    \n",
    "    # labels - need to one-hot encode TOP - K label\n",
    "    labels = one_hot_y(labels,label_feature_size)\n",
    "    labels = np.vstack(labels)\n",
    "    return frames,frames_audio, video_rgb,video_audio, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio, train_labels = \\\n",
    "    transform_data_for_lstm(train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 10, 1024), (16, 10, 128), (16, 1024), (16, 128), (16, 10))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame_rgb.shape, train_frame_audio.shape, train_video_rgb.shape, train_video_audio.shape,train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio, val_labels = \\\n",
    "    transform_data_for_lstm( val_video_rgb, val_video_audio,val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 10, 1024), (4, 10, 128), (4, 1024), (4, 128), (4, 10))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_frame_rgb.shape, val_frame_audio.shape, val_video_rgb.shape, val_video_audio.shape,val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 7s 408ms/step - loss: 3.8839 - acc: 0.3750 - val_loss: 18.3893 - val_acc: 0.2500\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.8130 - acc: 0.4375 - val_loss: 18.3122 - val_acc: 0.2500\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.7296 - acc: 0.4375 - val_loss: 18.2211 - val_acc: 0.2500\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6801 - acc: 0.4375 - val_loss: 18.2211 - val_acc: 0.2500\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.7411 - acc: 0.4375 - val_loss: 18.2666 - val_acc: 0.2500\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 3.6986 - acc: 0.4375 - val_loss: 18.2108 - val_acc: 0.2500\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6579 - acc: 0.4375 - val_loss: 18.2666 - val_acc: 0.2500\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6677 - acc: 0.4375 - val_loss: 14.7621 - val_acc: 0.2500\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6817 - acc: 0.4375 - val_loss: 18.2879 - val_acc: 0.2500\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6620 - acc: 0.4375 - val_loss: 18.2879 - val_acc: 0.2500\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6794 - acc: 0.4375 - val_loss: 18.2936 - val_acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e71ba3710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "\n",
    "checkpoint_dir = 'checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "bst_model_path = checkpoint_dir + STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n",
    "\n",
    "model.fit([train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels,\n",
    "          validation_data=([val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio], val_labels),\n",
    "          epochs=200, batch_size=1, shuffle=True, callbacks=[early_stopping, model_checkpoint, tensorboard]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
