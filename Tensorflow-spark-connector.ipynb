{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt8pm  yt8pm_100th_shard\r\n"
     ]
    }
   ],
   "source": [
    "# check access to S3 bucket, if empty, need to remount with:\n",
    "# `sudo s3fs cs205-youtube-data -o use_cache=/tmp -o allow_other -o uid=1001 -o mp_umask=002 -o multireq_max=5 mys3bucket`\n",
    "!ls mys3bucket/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('TFrecords_loading').set(\"spark.jars\", \"ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFirst, we need to install yum (make sure you are in correct environment!):\\nsudo apt install yum\\n\\nThen,\\n\\nInstall Maven:\\nIcarus answered a very similar question for me. Its not using \"yum\", but should still work for your purposes. Try,\\n\\nwget http://mirrors.advancedhosters.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz\\nbasically just go to the maven site. Find the version of maven you want. The file type and use the mirror for the wget statement above.\\n\\nAfterwards the process is easy\\n\\nRun the wget command from the dir you want to extract maven too.\\nrun the following to extract the tar,\\n\\ntar xvf apache-maven-3.0.5-bin.tar.gz\\nmove maven to /usr/local/apache-maven\\n\\nmv apache-maven-3.0.5  /usr/local/apache-maven\\nNext add the env variables to your ~/.bashrc file\\n\\n## note - the following commands need to be re-executed when re-starting the instance.\\n\\nexport M2_HOME=/usr/local/apache-maven\\nexport M2=$M2_HOME/bin \\nexport PATH=$M2:$PATH\\nExecute these commands\\n\\nsource ~/.bashrc\\n\\nTHEN\\ngit clone the following repo\\ngit clone https://github.com/tensorflow/ecosystem.git\\n\\ncd ecosystem/hadoop\\nmvn clean package\\n\\nthen\\ncd ../../hadoop #or make sure we are in the hadoop directory\\nmvn clean install\\n\\n# Build Spark TensorFlow connector\\ncd ../spark/spark-tensorflow-connector\\nmvn clean install\\n\\nand run spark with spark-tensorflow connector!\\npyspark --jars ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "First, we need to install yum (make sure you are in correct environment!):\n",
    "sudo apt install yum\n",
    "\n",
    "Then,\n",
    "\n",
    "Install Maven:\n",
    "Icarus answered a very similar question for me. Its not using \"yum\", but should still work for your purposes. Try,\n",
    "\n",
    "wget http://mirrors.advancedhosters.com/apache/maven/maven-3/3.6.1/binaries/apache-maven-3.6.1-bin.tar.gz\n",
    "basically just go to the maven site. Find the version of maven you want. The file type and use the mirror for the wget statement above.\n",
    "\n",
    "Afterwards the process is easy\n",
    "\n",
    "Run the wget command from the dir you want to extract maven too.\n",
    "run the following to extract the tar,\n",
    "\n",
    "tar xvf apache-maven-3.0.5-bin.tar.gz\n",
    "move maven to /usr/local/apache-maven\n",
    "\n",
    "mv apache-maven-3.0.5  /usr/local/apache-maven\n",
    "Next add the env variables to your ~/.bashrc file\n",
    "\n",
    "## note - the following commands need to be re-executed when re-starting the instance.\n",
    "\n",
    "export M2_HOME=/usr/local/apache-maven\n",
    "export M2=$M2_HOME/bin \n",
    "export PATH=$M2:$PATH\n",
    "Execute these commands\n",
    "\n",
    "source ~/.bashrc\n",
    "\n",
    "THEN\n",
    "git clone the following repo\n",
    "git clone https://github.com/tensorflow/ecosystem.git\n",
    "\n",
    "cd ecosystem/hadoop\n",
    "mvn clean package\n",
    "\n",
    "then\n",
    "cd ../../hadoop #or make sure we are in the hadoop directory\n",
    "mvn clean install\n",
    "\n",
    "# Build Spark TensorFlow connector\n",
    "cd ../spark/spark-tensorflow-connector\n",
    "mvn clean install\n",
    "\n",
    "and run spark with spark-tensorflow connector!\n",
    "pyspark --jars ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r test-output.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+----------+--------+----------+---+\n",
      "|LongCol|DoubleCol|StringCol|IntegerCol|FloatCol| VectorCol| id|\n",
      "+-------+---------+---------+----------+--------+----------+---+\n",
      "|     23|     14.0|       r1|         1|    10.0|[1.0, 2.0]| 11|\n",
      "|     24|     15.0|       r2|         2|    12.0|[2.0, 2.0]| 21|\n",
      "+-------+---------+---------+----------+--------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the example below is taken from:\n",
    "\n",
    "# https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-connector#usage-examples\n",
    "\n",
    "path = \"test-output.tfrecord\"\n",
    "\n",
    "fields = [StructField(\"id\", IntegerType()), StructField(\"IntegerCol\", IntegerType()),\n",
    "          StructField(\"LongCol\", LongType()), StructField(\"FloatCol\", FloatType()),\n",
    "          StructField(\"DoubleCol\", DoubleType()), StructField(\"VectorCol\", ArrayType(DoubleType(), True)),\n",
    "          StructField(\"StringCol\", StringType())]\n",
    "schema = StructType(fields)\n",
    "test_rows = [[11, 1, 23, 10.0, 14.0, [1.0, 2.0], \"r1\"], [21, 2, 24, 12.0, 15.0, [2.0, 2.0], \"r2\"]]\n",
    "rdd = spark.sparkContext.parallelize(test_rows)\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.write.format(\"tfrecords\").option(\"recordType\", \"Example\").save(path)\n",
    "\n",
    "df = spark.read.format(\"tfrecords\").option(\"recordType\", \"Example\").load(path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
