{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try just using Video-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_files(video_files_path):    \n",
    "    '''\n",
    "    Extraction of Youtube tfrecords video file features.\n",
    "    \n",
    "    Args: path to video files (note: developed with assumption of storing on s3 bucket and assessing with glob)\n",
    "    \n",
    "    Assumes each video in the tfrecord has following features:\n",
    "    'id' : bytes_list\n",
    "    'labels' : int64_list\n",
    "    'mean_rgb': float_list\n",
    "    'mean_audio': float_list\n",
    "    \n",
    "    returns:\n",
    "    numpy arrays of video ids, video multi-labels, mean rgb and mean audio\n",
    "    '''\n",
    "    \n",
    "    vid_ids = []\n",
    "    labels = []\n",
    "    mean_rgb = []\n",
    "    mean_audio = []\n",
    "\n",
    "    for file in tqdm(glob(video_files_path)):\n",
    "        for example in tf.python_io.tf_record_iterator(file):\n",
    "            tf_example = tf.train.Example.FromString(example)\n",
    "\n",
    "            vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "            labels.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "            mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n",
    "            mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n",
    "            \n",
    "    assert len(vid_ids) == len(labels),\"The number of IDs does not match the number of labeled videos.\"\n",
    "    return vid_ids, labels, mean_rgb, mean_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-adf8daf949ef>:23: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:04<00:00, 10.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# note: this will ONLY work with small datasets, for large ones, we need to create iterators\n",
    "train_vid_ids, train_labels, train_mean_rgb, train_mean_audio = extract_video_files(\"mys3bucket/yt8pm_100th_shard/v2/video/train*\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41393, 41393)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vid_ids),len(train_labels),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 24.27it/s]\n"
     ]
    }
   ],
   "source": [
    "test_vid_ids, test_labels, test_mean_rgb, test_mean_audio = extract_video_files(\"mys3bucket/yt8pm_100th_shard/v2/video/test*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:01<00:00, 22.30it/s]\n"
     ]
    }
   ],
   "source": [
    "val_vid_ids, val_labels, val_mean_rgb, val_mean_audio = extract_video_files(\"mys3bucket/yt8pm_100th_shard/v2/video/validate*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract validation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame_level_features(frame_files_path,maximum_iter = False,stop_at_iter = 10,num_tf_records=1):\n",
    "    '''\n",
    "    Extraction of Youtube tfrecords frame file features.\n",
    "    \n",
    "    Args: \n",
    "    path to video files (note: developed with assumption of storing on s3 bucket and assessing with glob)\n",
    "    \n",
    "    maximum_iter - flag- if True, will limit number of videos extracted from each TF record\n",
    "    stop_at_iter - number of videos to extract\n",
    "    num_tf_records - number of records to extract - WARNING!!! this is VERY slow, if bigger than 1\n",
    "    \n",
    "    Assumes each video in the tfrecord has following features:\n",
    "    'id' : bytes_list\n",
    "    'labels' : int64_list\n",
    "    'audio': float arr, each frame 128\n",
    "    'rgb', float arr, each frame 1024\n",
    "    \n",
    "    returns:\n",
    "    numpy arrays of frame ids, frame multi-labels, frame audio, frame rgb\n",
    "    '''\n",
    "    frame_ids = []\n",
    "    frame_labels = []\n",
    "    feat_rgb = []\n",
    "    feat_audio = []\n",
    "    # ATTENTION: only use one TF record for debugging.\n",
    "    for file in tqdm(glob(frame_files_path)[:num_tf_records]):\n",
    "        print(f'There is {sum(1 for _ in tf.python_io.tf_record_iterator(file))} videos in this TF record.')\n",
    "        iter_ = 0\n",
    "        for example in tf.python_io.tf_record_iterator(file):\n",
    "            if maximum_iter and iter_==stop_at_iter:\n",
    "                break\n",
    "            tf_example = tf.train.Example.FromString(example)\n",
    "\n",
    "            frame_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "            frame_labels.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "            \n",
    "            tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
    "            n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "            \n",
    "            rgb_frame = []\n",
    "            audio_frame = []\n",
    "\n",
    "            # iterate through frames\n",
    "            sys.stdout.flush()\n",
    "            for i in range(n_frames):\n",
    "                sess = tf.InteractiveSession()\n",
    "                sys.stdout.write('\\r'+'iterating video: ' + str(iter_)+ ' ,frames: ' + str(i)+'/'+str(n_frames))\n",
    "                sys.stdout.flush()\n",
    "                rgb_frame.append(tf.cast(tf.decode_raw(\n",
    "                        tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                               ,tf.float32).eval())\n",
    "                audio_frame.append(tf.cast(tf.decode_raw(\n",
    "                        tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                               ,tf.float32).eval())\n",
    "\n",
    "                tf.reset_default_graph()\n",
    "                sess.close()\n",
    "            feat_rgb.append(rgb_frame)\n",
    "            feat_audio.append(audio_frame)\n",
    "            iter_+=1\n",
    "        \n",
    "    return frame_ids, frame_labels, feat_rgb, feat_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1015 videos in this TF record.\n",
      "iterating video: 19 ,frames: 220/221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:53<00:00, 53.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# this will cause out-of-memory issues when ran on the WHOLE data set\n",
    "# change it so that train and test are loaded one shard at a time\n",
    "train_frame_ids, train_frame_labels,train_frame_rgb,train_frame_audio \\\n",
    "= extract_frame_level_features(\"mys3bucket/yt8pm_100th_shard/v2/frame/train*\",maximum_iter=True,\\\n",
    "                               stop_at_iter=20,num_tf_records=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only grabbed 5 videos, it took 6 minutes to load!!\n",
    "len(train_frame_ids),len(train_frame_labels),len(train_frame_rgb),len(train_frame_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first video, first frame has shape 1024...\n",
    "train_frame_rgb[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM video classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import operator\n",
    "import time \n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating training and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Model parameters and creating architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10 class problem for now?\n",
    "label_feature_size = 10\n",
    "\n",
    "# how many frames we will use from each video?\n",
    "max_frame_rgb_sequence_length = 10\n",
    "frame_rgb_embedding_size = 1024\n",
    "\n",
    "# how many audio sequences we will use from each video?\n",
    "max_frame_audio_sequence_length = 10\n",
    "frame_audio_embedding_size = 128\n",
    "\n",
    "number_dense_units = 1000\n",
    "number_lstm_units = 100\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "activation_function='relu'\n",
    "validation_split_ratio = 0.2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n",
    "    \n",
    "    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n",
    "    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    \n",
    "    # creating input layer for frame-level data\n",
    "    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n",
    "    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n",
    "    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n",
    "    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n",
    "    \n",
    "    #creating input layer for video-level data \n",
    "    vid_shape=(1024,)\n",
    "    video_rgb_input = Input(shape=vid_shape)\n",
    "    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function, input_shape=vid_shape)(video_rgb_input)\n",
    "    \n",
    "    aud_shape=(128,)\n",
    "    video_audio_input = Input(shape=aud_shape)\n",
    "    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function,input_shape = aud_shape)(video_audio_input)\n",
    "    \n",
    "    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n",
    "    merged_frame = concatenate([frame_x1, frame_x2])\n",
    "    merged_frame = BatchNormalization()(merged_frame)\n",
    "    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n",
    "    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n",
    "    \n",
    "    # merging video-level dense layer output\n",
    "    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n",
    "    merged_video = BatchNormalization()(video_rgb_dense)\n",
    "    merged_video = Dropout(rate_drop_dense)(merged_video)\n",
    "    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n",
    "\n",
    "    \n",
    "    # merging frame-level and video-level dense layer output\n",
    "    merged = concatenate([merged_frame_dense, merged_video_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "     \n",
    "    merged = Dense(number_dense_units, activation=activation_function)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          900000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          183200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          512500      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400)          1600        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500)          2000        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          200500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 500)          250500      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000)         0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1000)         4000        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1000)         1001000     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000)         4000        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           10010       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,069,310\n",
      "Trainable params: 3,063,510\n",
      "Non-trainable params: 5,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41393, 41393, 41393, 41393)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vid_ids), len(train_labels), len(train_mean_rgb), len(train_mean_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only grabbed 5 videos, it took 6 minutes to load!!\n",
    "len(train_frame_ids),len(train_frame_labels),len(train_frame_rgb),len(train_frame_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATbF',\n",
       " 'q2bF',\n",
       " 'exbF',\n",
       " 'gNbF',\n",
       " 'GkbF',\n",
       " 'aUbF',\n",
       " 'A9bF',\n",
       " 'UGbF',\n",
       " 'pUbF',\n",
       " 'DLbF',\n",
       " '52bF',\n",
       " 'CsbF',\n",
       " '3lbF',\n",
       " '6ZbF',\n",
       " 'rubF',\n",
       " 'UqbF',\n",
       " 'HebF',\n",
       " 'oJbF',\n",
       " 'lKbF',\n",
       " 'CFbF']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_frame_ids).intersection(set(train_vid_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ATbF',\n",
       " 'q2bF',\n",
       " 'exbF',\n",
       " 'gNbF',\n",
       " 'GkbF',\n",
       " 'aUbF',\n",
       " 'A9bF',\n",
       " 'UGbF',\n",
       " 'pUbF',\n",
       " 'DLbF',\n",
       " '52bF',\n",
       " 'CsbF',\n",
       " '3lbF',\n",
       " '6ZbF',\n",
       " 'rubF',\n",
       " 'UqbF',\n",
       " 'HebF',\n",
       " 'oJbF',\n",
       " 'lKbF',\n",
       " 'CFbF']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_frame_ids).intersection(set(train_vid_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_dataset(video_rgb, video_audio, vid_ids, frame_rgb, frame_audio, frame_labels, frame_ids):\n",
    "    \"\"\"\n",
    "    Method to created training and validation data. \n",
    "    We need to make sure we only use video IDs for which we have frames.\n",
    "    This is handled below.\n",
    "    \n",
    "    \"\"\"\n",
    "    # we have to have the same video of for both video and frame-level features\n",
    "    video_rgb_matching = []\n",
    "    video_audio_matching = []\n",
    "    \n",
    "    for idx in frame_ids: # for each ID available on frame level, find matching video-level features\n",
    "        for i, idx_vid in enumerate(vid_ids): # scan through video-level ids\n",
    "            if idx == idx_vid: \n",
    "                video_rgb_matching.append(video_rgb[i])\n",
    "                video_audio_matching.append(video_audio[i])\n",
    "                \n",
    "                \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(frame_labels)))\n",
    "        \n",
    "    video_rgb_shuffled = np.array(video_rgb_matching)[shuffle_indices]\n",
    "    video_audio_shuffled = np.array(video_audio_matching)[shuffle_indices]\n",
    "    frame_rgb_shuffled = np.array(frame_rgb)[shuffle_indices]\n",
    "    frame_audio_shuffled = np.array(frame_audio)[shuffle_indices]\n",
    "    labels_shuffled = np.array(frame_labels)[shuffle_indices]\n",
    "\n",
    "    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n",
    "    \n",
    "    # delete orig vars to clear some cache\n",
    "    del video_rgb\n",
    "    del video_audio\n",
    "    del frame_rgb\n",
    "    del frame_audio\n",
    "    gc.collect()\n",
    "    \n",
    "    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n",
    "    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n",
    "    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n",
    "    \n",
    "    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n",
    "    gc.collect()\n",
    "    \n",
    "    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, \n",
    "            val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, \\\n",
    "train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels \\\n",
    "                = create_train_dev_dataset(train_mean_rgb, train_mean_audio, train_vid_ids, train_frame_rgb, \\\n",
    "                                            train_frame_audio, train_frame_labels, train_frame_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into final input in the model\n",
    "def one_hot_y(raw_labels=train_labels,label_size=20):\n",
    "    '''\n",
    "    Helper function to transform labels into one-hot TOP 20\n",
    "    Uses np.unique(return_counts=True) as implicit sorter (first K labels are the most frequent)\n",
    "    '''\n",
    "    all_labels = []\n",
    "    for i in list(raw_labels):\n",
    "        for j in list(i):\n",
    "            all_labels.append(j)\n",
    "\n",
    "    results = np.unique(all_labels,return_counts=True)\n",
    "    labels_vocab,counts = results\n",
    "\n",
    "    labels = labels_vocab[:label_size-1] #last columns will be 1 if none of those labels found in a video\n",
    "    output = []\n",
    "    for set_of_labels in raw_labels:\n",
    "        \n",
    "        # preallocate numpy arr for each set of labels\n",
    "        sequence = np.zeros(label_size)\n",
    "        # loop through all the labels in one video and flip them to 1s\n",
    "        for this_label in set_of_labels:\n",
    "            designation = np.where(labels==this_label)\n",
    "            for des in designation:\n",
    "                sequence[des]=1\n",
    "        # done with one training points\n",
    "        if sequence.sum()==0:\n",
    "            sequence[-1]=1\n",
    "        output.append(sequence)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_lstm(video_rgb,video_audio, frame_rgb, frame_audio,\n",
    "                            labels,label_feature_size=10,max_frame_rgb_sequence_length = 10,\\\n",
    "                            max_frame_audio_sequence_length = 10):\n",
    "    frames = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_rgb_sequence_length x 1024)\n",
    "    #print(len(frame_rgb))\n",
    "    \n",
    "    for frame in frame_rgb: \n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames.append(np.vstack(frame)[:max_frame_rgb_sequence_length,:])\n",
    "    #print(len(frames))\n",
    "\n",
    "    frames = np.reshape(np.array(frames),(len(frame_rgb),max_frame_rgb_sequence_length,1024))\n",
    "\n",
    "    #print(frames.shape)\n",
    "    \n",
    "    frames_audio = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_audio_sequence_length x 128)\n",
    "    for frame in frame_audio:\n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames_audio.append(np.vstack(frame)[:max_frame_audio_sequence_length,:])\n",
    "\n",
    "    frames_audio = np.reshape(np.array(frames_audio),(len(frame_audio),max_frame_audio_sequence_length,128))\n",
    "    #print(frames_audio.shape)\n",
    "    \n",
    "    # deal with videos\n",
    "    \n",
    "    video_rgb = np.vstack(video_rgb)\n",
    "    video_audio = np.vstack(video_audio)\n",
    "    \n",
    "    \n",
    "    # labels - need to one-hot encode TOP - K label\n",
    "    labels = one_hot_y(labels,label_feature_size)\n",
    "    labels = np.vstack(labels)\n",
    "    return frames,frames_audio, video_rgb,video_audio, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio, train_labels = \\\n",
    "    transform_data_for_lstm(train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16, 10, 1024), (16, 10, 128), (16, 1024), (16, 128), (16, 10))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame_rgb.shape, train_frame_audio.shape, train_video_rgb.shape, train_video_audio.shape,train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio, val_labels = \\\n",
    "    transform_data_for_lstm( val_video_rgb, val_video_audio,val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 10, 1024), (4, 10, 128), (4, 1024), (4, 128), (4, 10))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_frame_rgb.shape, val_frame_audio.shape, val_video_rgb.shape, val_video_audio.shape,val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16 samples, validate on 4 samples\n",
      "Epoch 1/200\n",
      "16/16 [==============================] - 7s 408ms/step - loss: 3.8839 - acc: 0.3750 - val_loss: 18.3893 - val_acc: 0.2500\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.8130 - acc: 0.4375 - val_loss: 18.3122 - val_acc: 0.2500\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.7296 - acc: 0.4375 - val_loss: 18.2211 - val_acc: 0.2500\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6801 - acc: 0.4375 - val_loss: 18.2211 - val_acc: 0.2500\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.7411 - acc: 0.4375 - val_loss: 18.2666 - val_acc: 0.2500\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 3.6986 - acc: 0.4375 - val_loss: 18.2108 - val_acc: 0.2500\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6579 - acc: 0.4375 - val_loss: 18.2666 - val_acc: 0.2500\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6677 - acc: 0.4375 - val_loss: 14.7621 - val_acc: 0.2500\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 3.6817 - acc: 0.4375 - val_loss: 18.2879 - val_acc: 0.2500\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6620 - acc: 0.4375 - val_loss: 18.2879 - val_acc: 0.2500\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.6794 - acc: 0.4375 - val_loss: 18.2936 - val_acc: 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e71ba3710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "    \n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f' % (number_lstm_units, number_dense_units, rate_drop_lstm, rate_drop_dense)\n",
    "\n",
    "checkpoint_dir = 'checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "bst_model_path = checkpoint_dir + STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))\n",
    "\n",
    "model.fit([train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels,\n",
    "          validation_data=([val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio], val_labels),\n",
    "          epochs=200, batch_size=1, shuffle=True, callbacks=[early_stopping, model_checkpoint, tensorboard]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
