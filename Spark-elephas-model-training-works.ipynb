{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import multiprocessing as mp # if we want to parallelize i/o\n",
    "\n",
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import operator\n",
    "import time \n",
    "import gc\n",
    "import os\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from elephas.spark_model import SparkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Youtube-8M') \\\n",
    "                  .set(\"spark.jars\",\n",
    "                       \"ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train0093.tfrecord-converted.tfrecord  train0208.tfrecord-converted.tfrecord\r\n",
      "train0111.tfrecord-converted.tfrecord  train0434.tfrecord-converted.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!ls mys3bucket/converted_records_for_spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"tfrecords\").option(\"recordType\", \"SequenceExample\").load('mys3bucket/converted_records_for_spark/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4029"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          mean_audio|              labels|            mean_rgb|           frame_rgb|         frame_audio|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[-1.185913, -0.84...|[0.0, 0.0, 1.0, 0...|[0.53471446, 1.00...|[WrappedArray(238...|[WrappedArray(86....|\n",
      "|[0.9230174, -0.34...|[0.0, 0.0, 0.0, 0...|[-0.10382305, -1....|[WrappedArray(51....|[WrappedArray(192...|\n",
      "|[-0.32460678, -0....|[0.0, 0.0, 0.0, 0...|[-0.5026991, -1.6...|[WrappedArray(0.0...|[WrappedArray(173...|\n",
      "|[0.9312072, -0.73...|[0.0, 0.0, 0.0, 0...|[-0.6032936, -0.4...|[WrappedArray(0.0...|[WrappedArray(163...|\n",
      "|[0.11735498, 1.13...|[0.0, 0.0, 0.0, 0...|[0.61926347, 0.20...|[WrappedArray(121...|[WrappedArray(121...|\n",
      "|[-0.83108944, -1....|[0.0, 0.0, 0.0, 0...|[0.48174715, 0.67...|[WrappedArray(73....|[WrappedArray(75....|\n",
      "|[-0.6435339, -0.6...|[1.0, 1.0, 0.0, 0...|[0.024073938, -0....|[WrappedArray(0.0...|[WrappedArray(177...|\n",
      "|[-1.6815804, 1.89...|[1.0, 0.0, 0.0, 0...|[0.13661005, -1.3...|[WrappedArray(146...|[WrappedArray(50....|\n",
      "|[1.294819, 1.5030...|[0.0, 0.0, 0.0, 1...|[0.4091197, -0.62...|[WrappedArray(153...|[WrappedArray(190...|\n",
      "|[1.175444, 1.8621...|[0.0, 0.0, 0.0, 1...|[-0.8292868, 0.38...|[WrappedArray(25....|[WrappedArray(194...|\n",
      "|[0.039699353, 0.4...|[0.0, 0.0, 0.0, 0...|[0.43012044, -1.5...|[WrappedArray(110...|[WrappedArray(173...|\n",
      "|[-0.3033509, -0.6...|[0.0, 0.0, 0.0, 0...|[0.28378636, 0.80...|[WrappedArray(0.0...|[WrappedArray(129...|\n",
      "|[-1.2610502, 1.55...|[1.0, 0.0, 0.0, 0...|[-0.13540319, 0.1...|[WrappedArray(120...|[WrappedArray(91....|\n",
      "|[-0.66810906, -0....|[0.0, 0.0, 0.0, 0...|[0.83819157, 1.09...|[WrappedArray(0.0...|[WrappedArray(136...|\n",
      "|[0.8888252, -0.41...|[1.0, 1.0, 0.0, 0...|[1.269664, 0.0225...|[WrappedArray(165...|[WrappedArray(178...|\n",
      "|[-1.3135805, 0.76...|[0.0, 0.0, 0.0, 0...|[0.21302176, 0.91...|[WrappedArray(172...|[WrappedArray(14....|\n",
      "|[-0.71007967, -0....|[0.0, 0.0, 0.0, 0...|[-0.28900123, 0.1...|[WrappedArray(0.0...|[WrappedArray(173...|\n",
      "|[-1.155377, -0.57...|[0.0, 0.0, 1.0, 0...|[0.3896687, -0.54...|[WrappedArray(0.0...|[WrappedArray(173...|\n",
      "|[-0.16980188, -0....|[1.0, 0.0, 0.0, 0...|[-0.09913194, 0.4...|[WrappedArray(108...|[WrappedArray(164...|\n",
      "|[0.45434844, 0.46...|[1.0, 1.0, 0.0, 0...|[0.12007394, -0.5...|[WrappedArray(0.0...|[WrappedArray(140...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the order of vars to fit the order in the model!\n",
    "# train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels\n",
    "# 3, 4, 2, 0 , 1\n",
    "\n",
    "# NOTE: ELEPHAS DOES NOT SUPPORT MULTI-INPUT OR THE FORMATTING IS NOT CORRECT.\n",
    "train_rdd_new = train_rdd.map(lambda x: ((np.array(x[3]),np.array(x[4]),np.array(x[2]),np.array(x[0])),np.array(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this works - only frame-level RGB\n",
    "train_rdd_simple = train_rdd.map(lambda x: (np.array(x[3]),np.array(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_rdd_new.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#SPECIFY PARAMS FIRST\n",
    "###########\n",
    "\n",
    "# 1000 class problem for now?\n",
    "label_feature_size = 10\n",
    "\n",
    "# how many frames we will use from each video?\n",
    "max_frame_rgb_sequence_length = 10\n",
    "frame_rgb_embedding_size = 1024\n",
    "\n",
    "# how many audio sequences we will use from each video?\n",
    "max_frame_audio_sequence_length = 10\n",
    "frame_audio_embedding_size = 128\n",
    "\n",
    "number_dense_units = 1000\n",
    "number_lstm_units = 100\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "activation_function='relu'\n",
    "validation_split_ratio = 0 # to use all\n",
    "\n",
    "def create_model_simple():\n",
    "    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n",
    "    \n",
    "    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n",
    "    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    #lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    \n",
    "    # creating input layer for frame-level data\n",
    "    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n",
    "    #frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n",
    "    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n",
    "    #frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n",
    "    \n",
    "    #creating input layer for video-level data \n",
    "#     vid_shape=(1024,)\n",
    "#     video_rgb_input = Input(shape=vid_shape)\n",
    "#     video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function, input_shape=vid_shape)(video_rgb_input)\n",
    "    \n",
    "#     aud_shape=(128,)\n",
    "#     video_audio_input = Input(shape=aud_shape)\n",
    "#     video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function,input_shape = aud_shape)(video_audio_input)\n",
    "    \n",
    "    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n",
    "    merged_frame = frame_x1#concatenate([frame_x1, frame_x2])\n",
    "    merged_frame = BatchNormalization()(merged_frame)\n",
    "    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n",
    "    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n",
    "    \n",
    "#     # merging video-level dense layer output\n",
    "#     merged_video = concatenate([video_rgb_dense, video_audio_dense])\n",
    "#     merged_video = BatchNormalization()(video_rgb_dense)\n",
    "#     merged_video = Dropout(rate_drop_dense)(merged_video)\n",
    "#     merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n",
    "\n",
    "    \n",
    "    # merging frame-level and video-level dense layer output\n",
    "    merged = merged_frame_dense#concatenate([merged_frame_dense, merged_video_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "     \n",
    "    merged = Dense(number_dense_units, activation=activation_function)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[frame_rgb_sequence_input], outputs=preds)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n",
    "    \n",
    "    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n",
    "    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    \n",
    "    # creating input layer for frame-level data\n",
    "    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n",
    "    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n",
    "    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n",
    "    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n",
    "    \n",
    "    #creating input layer for video-level data \n",
    "    vid_shape=(1024,)\n",
    "    video_rgb_input = Input(shape=vid_shape)\n",
    "    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function, input_shape=vid_shape)(video_rgb_input)\n",
    "    \n",
    "    aud_shape=(128,)\n",
    "    video_audio_input = Input(shape=aud_shape)\n",
    "    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function,input_shape = aud_shape)(video_audio_input)\n",
    "    \n",
    "    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n",
    "    merged_frame = concatenate([frame_x1, frame_x2])\n",
    "    merged_frame = BatchNormalization()(merged_frame)\n",
    "    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n",
    "    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n",
    "    \n",
    "    # merging video-level dense layer output\n",
    "    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n",
    "    merged_video = BatchNormalization()(video_rgb_dense)\n",
    "    merged_video = Dropout(rate_drop_dense)(merged_video)\n",
    "    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n",
    "\n",
    "    \n",
    "    # merging frame-level and video-level dense layer output\n",
    "    merged = concatenate([merged_frame_dense, merged_video_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "     \n",
    "    merged = Dense(number_dense_units, activation=activation_function)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_28 (InputLayer)        (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, 200)               900000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,518,310\n",
      "Trainable params: 1,514,910\n",
      "Non-trainable params: 3,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "keras_model= create_model_simple()\n",
    "#keras_model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model = SparkModel(keras_model, frequency='batch', mode='synchronous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-cf866cf8b2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rdd_simple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, epochs, batch_size, verbose, validation_split)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, rdd, epochs, batch_size, verbose, validation_split)\u001b[0m\n\u001b[1;32m    186\u001b[0m             worker = SparkWorker(yaml, parameters, train_config,\n\u001b[1;32m    187\u001b[0m                                  optimizer, loss, metrics, custom)\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mnew_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simply accumulate gradients one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "history = spark_model.fit(train_rdd_simple, epochs=10, batch_size=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
