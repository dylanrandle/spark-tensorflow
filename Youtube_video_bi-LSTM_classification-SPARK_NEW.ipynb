{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM video classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from IPython.display import YouTubeVideo\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import multiprocessing as mp # if we want to parallelize i/o\n",
    "\n",
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "import operator\n",
    "import time \n",
    "import gc\n",
    "import os\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "import numpy as np\n",
    "from elephas.spark_model import SparkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "#SPECIFY PARAMS FIRST\n",
    "###########\n",
    "\n",
    "# 10 class problem for now?\n",
    "label_feature_size = 10\n",
    "\n",
    "# how many frames we will use from each video?\n",
    "max_frame_rgb_sequence_length = 10\n",
    "frame_rgb_embedding_size = 1024\n",
    "\n",
    "# how many audio sequences we will use from each video?\n",
    "max_frame_audio_sequence_length = 10\n",
    "frame_audio_embedding_size = 128\n",
    "\n",
    "number_dense_units = 1000\n",
    "number_lstm_units = 100\n",
    "rate_drop_lstm = 0.2\n",
    "rate_drop_dense = 0.2\n",
    "activation_function='relu'\n",
    "validation_split_ratio = 0.2\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \"\"\"Create and store best model at `checkpoint` path ustilising bi-lstm layer for frame level data of videos\"\"\"\n",
    "    \n",
    "    # Creating 2 bi-lstm layer, one for rgb and other for audio level data\n",
    "    lstm_layer_1 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    lstm_layer_2 = Bidirectional(LSTM(number_lstm_units, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "    \n",
    "    # creating input layer for frame-level data\n",
    "    frame_rgb_sequence_input = Input(shape=(max_frame_rgb_sequence_length, frame_rgb_embedding_size), dtype='float32')\n",
    "    frame_audio_sequence_input = Input(shape=(max_frame_audio_sequence_length, frame_audio_embedding_size), dtype='float32')\n",
    "    frame_x1 = lstm_layer_1(frame_rgb_sequence_input)\n",
    "    frame_x2 = lstm_layer_2(frame_audio_sequence_input)\n",
    "    \n",
    "    #creating input layer for video-level data \n",
    "    vid_shape=(1024,)\n",
    "    video_rgb_input = Input(shape=vid_shape)\n",
    "    video_rgb_dense = Dense(int(number_dense_units/2), activation=activation_function, input_shape=vid_shape)(video_rgb_input)\n",
    "    \n",
    "    aud_shape=(128,)\n",
    "    video_audio_input = Input(shape=aud_shape)\n",
    "    video_audio_dense = Dense(int(number_dense_units/2), activation=activation_function,input_shape = aud_shape)(video_audio_input)\n",
    "    \n",
    "    # merging frame-level bi-lstm output and later passed to dense layer by applying batch-normalisation and dropout\n",
    "    merged_frame = concatenate([frame_x1, frame_x2])\n",
    "    merged_frame = BatchNormalization()(merged_frame)\n",
    "    merged_frame = Dropout(rate_drop_dense)(merged_frame)\n",
    "    merged_frame_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_frame)\n",
    "    \n",
    "    # merging video-level dense layer output\n",
    "    merged_video = concatenate([video_rgb_dense, video_audio_dense])\n",
    "    merged_video = BatchNormalization()(video_rgb_dense)\n",
    "    merged_video = Dropout(rate_drop_dense)(merged_video)\n",
    "    merged_video_dense = Dense(int(number_dense_units/2), activation=activation_function)(merged_video)\n",
    "\n",
    "    \n",
    "    # merging frame-level and video-level dense layer output\n",
    "    merged = concatenate([merged_frame_dense, merged_video_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "     \n",
    "    merged = Dense(number_dense_units, activation=activation_function)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(rate_drop_dense)(merged)\n",
    "    preds = Dense(label_feature_size, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[frame_rgb_sequence_input, frame_audio_sequence_input, video_rgb_input, video_audio_input], outputs=preds)\n",
    "\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_files(video_files_path):    \n",
    "    '''\n",
    "    Extraction of Youtube tfrecords video file features.\n",
    "    \n",
    "    Args: path to video files (note: developed with assumption of storing on s3 bucket and assessing with glob)\n",
    "    \n",
    "    Assumes each video in the tfrecord has following features:\n",
    "    'id' : bytes_list\n",
    "    'labels' : int64_list\n",
    "    'mean_rgb': float_list\n",
    "    'mean_audio': float_list\n",
    "    \n",
    "    returns:\n",
    "    numpy arrays of video ids, video multi-labels, mean rgb and mean audio\n",
    "    '''\n",
    "    \n",
    "    vid_ids = []\n",
    "    labels = []\n",
    "    mean_rgb = []\n",
    "    mean_audio = []\n",
    "\n",
    "    for file in tqdm(glob(video_files_path)):\n",
    "        for example in tf.python_io.tf_record_iterator(file):\n",
    "            tf_example = tf.train.Example.FromString(example)\n",
    "\n",
    "            vid_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "            labels.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "            mean_rgb.append(tf_example.features.feature['mean_rgb'].float_list.value)\n",
    "            mean_audio.append(tf_example.features.feature['mean_audio'].float_list.value)\n",
    "            \n",
    "    assert len(vid_ids) == len(labels),\"The number of IDs does not match the number of labeled videos.\"\n",
    "    return vid_ids, labels, mean_rgb, mean_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frame_level_features_per_tf_record(frame_file_path,maximum_iter = False,stop_at_iter = 10):\n",
    "    '''\n",
    "    Extraction of Youtube tfrecords frame file features.\n",
    "    \n",
    "    Args: \n",
    "    path to each tf_record (note: developed with assumption of storing on s3 bucket and assessing with glob)\n",
    "    \n",
    "    maximum_iter - flag- if True, will limit number of videos extracted from each TF record\n",
    "    stop_at_iter - number of videos to extract\n",
    "    num_tf_records - number of records to extract - WARNING!!! this is VERY slow, if bigger than 1\n",
    "    \n",
    "    Assumes each video in the tfrecord has following features:\n",
    "    'id' : bytes_list\n",
    "    'labels' : int64_list\n",
    "    'audio': float arr, each frame 128\n",
    "    'rgb', float arr, each frame 1024\n",
    "    \n",
    "    returns:\n",
    "    numpy arrays of frame ids, frame multi-labels, frame audio, frame rgb\n",
    "    '''\n",
    "    frame_ids = []\n",
    "    frame_labels = []\n",
    "    feat_rgb = []\n",
    "    feat_audio = []\n",
    "    # ATTENTION: only use one TF record for debugging.\n",
    "    print(f'There is {sum(1 for _ in tf.python_io.tf_record_iterator(frame_file_path))} videos in this TF record.')\n",
    "    iter_ = 0\n",
    "    for example in tf.python_io.tf_record_iterator(frame_file_path):\n",
    "        if maximum_iter and iter_==stop_at_iter:\n",
    "            break\n",
    "        tf_example = tf.train.Example.FromString(example)\n",
    "\n",
    "        frame_ids.append(tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8'))\n",
    "        frame_labels.append(tf_example.features.feature['labels'].int64_list.value)\n",
    "\n",
    "        tf_seq_example = tf.train.SequenceExample.FromString(example)\n",
    "        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n",
    "\n",
    "        rgb_frame = []\n",
    "        audio_frame = []\n",
    "\n",
    "        # iterate through frames\n",
    "        sys.stdout.flush()\n",
    "        for i in range(n_frames):\n",
    "            sess = tf.InteractiveSession()\n",
    "            sys.stdout.write('\\r'+'iterating video: ' + str(iter_)+ ' ,frames: ' + str(i)+'/'+str(n_frames))\n",
    "            sys.stdout.flush()\n",
    "            rgb_frame.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['rgb'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "            audio_frame.append(tf.cast(tf.decode_raw(\n",
    "                    tf_seq_example.feature_lists.feature_list['audio'].feature[i].bytes_list.value[0],tf.uint8)\n",
    "                           ,tf.float32).eval())\n",
    "\n",
    "            tf.reset_default_graph()\n",
    "            sess.close()\n",
    "        feat_rgb.append(rgb_frame)\n",
    "        feat_audio.append(audio_frame)\n",
    "        iter_+=1\n",
    "\n",
    "    return frame_ids, frame_labels, feat_rgb, feat_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_dataset(video_rgb, video_audio, vid_ids, frame_rgb, frame_audio, frame_labels, frame_ids):\n",
    "    \"\"\"\n",
    "    Method to created training and validation data. \n",
    "    We need to make sure we only use video IDs for which we have frames.\n",
    "    This is handled below.\n",
    "    \n",
    "    \"\"\"\n",
    "    # we have to have the same video of for both video and frame-level features\n",
    "    video_rgb_matching = []\n",
    "    video_audio_matching = []\n",
    "    \n",
    "    for idx in frame_ids: # for each ID available on frame level, find matching video-level features\n",
    "        for i, idx_vid in enumerate(vid_ids): # scan through video-level ids\n",
    "            if idx == idx_vid: \n",
    "                video_rgb_matching.append(video_rgb[i])\n",
    "                video_audio_matching.append(video_audio[i])\n",
    "                \n",
    "                \n",
    "    shuffle_indices = np.random.permutation(np.arange(len(frame_labels)))\n",
    "        \n",
    "    video_rgb_shuffled = np.array(video_rgb_matching)[shuffle_indices]\n",
    "    video_audio_shuffled = np.array(video_audio_matching)[shuffle_indices]\n",
    "    frame_rgb_shuffled = np.array(frame_rgb)[shuffle_indices]\n",
    "    frame_audio_shuffled = np.array(frame_audio)[shuffle_indices]\n",
    "    labels_shuffled = np.array(frame_labels)[shuffle_indices]\n",
    "\n",
    "    dev_idx = max(1, int(len(labels_shuffled) * validation_split_ratio))\n",
    "    \n",
    "    # delete orig vars to clear some cache\n",
    "    del video_rgb\n",
    "    del video_audio\n",
    "    del frame_rgb\n",
    "    del frame_audio\n",
    "    gc.collect()\n",
    "    \n",
    "    train_video_rgb, val_video_rgb = video_rgb_shuffled[:-dev_idx], video_rgb_shuffled[-dev_idx:]\n",
    "    train_video_audio, val_video_audio = video_audio_shuffled[:-dev_idx], video_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_frame_rgb, val_frame_rgb = frame_rgb_shuffled[:-dev_idx], frame_rgb_shuffled[-dev_idx:]\n",
    "    train_frame_audio, val_frame_audio = frame_audio_shuffled[:-dev_idx], frame_audio_shuffled[-dev_idx:]\n",
    "    \n",
    "    train_labels, val_labels = labels_shuffled[:-dev_idx], labels_shuffled[-dev_idx:]\n",
    "    \n",
    "    del video_rgb_shuffled, video_audio_shuffled, frame_rgb_shuffled, frame_audio_shuffled, labels_shuffled\n",
    "    gc.collect()\n",
    "    \n",
    "    return (train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, train_labels, val_video_rgb, val_video_audio, \n",
    "            val_frame_rgb, val_frame_audio, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into final input in the model\n",
    "def one_hot_y(raw_labels,label_size=20):\n",
    "    '''\n",
    "    Helper function to transform labels into one-hot TOP 20\n",
    "    Uses np.unique(return_counts=True) as implicit sorter (first K labels are the most frequent)\n",
    "    '''\n",
    "    all_labels = []\n",
    "    for i in list(raw_labels):\n",
    "        for j in list(i):\n",
    "            all_labels.append(j)\n",
    "\n",
    "    results = np.unique(all_labels,return_counts=True)\n",
    "    labels_vocab,counts = results\n",
    "\n",
    "    labels = labels_vocab[:label_size-1] #last columns will be 1 if none of those labels found in a video\n",
    "    output = []\n",
    "    for set_of_labels in raw_labels:\n",
    "        \n",
    "        # preallocate numpy arr for each set of labels\n",
    "        sequence = np.zeros(label_size)\n",
    "        # loop through all the labels in one video and flip them to 1s\n",
    "        for this_label in set_of_labels:\n",
    "            designation = np.where(labels==this_label)\n",
    "            for des in designation:\n",
    "                sequence[des]=1\n",
    "        # done with one training points\n",
    "        if sequence.sum()==0:\n",
    "            sequence[-1]=1\n",
    "        output.append(sequence)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_lstm(video_rgb,video_audio, frame_rgb, frame_audio,\n",
    "                            labels,label_feature_size=10,max_frame_rgb_sequence_length = 10,\\\n",
    "                            max_frame_audio_sequence_length = 10):\n",
    "    frames = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_rgb_sequence_length x 1024)\n",
    "    #print(len(frame_rgb))\n",
    "    \n",
    "    for frame in frame_rgb: \n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames.append(np.vstack(frame)[:max_frame_rgb_sequence_length,:])\n",
    "    #print(len(frames))\n",
    "\n",
    "    frames = np.reshape(np.array(frames),(len(frame_rgb),max_frame_rgb_sequence_length,1024))\n",
    "\n",
    "    #print(frames.shape)\n",
    "    \n",
    "    frames_audio = []\n",
    "    # need to transfrom to numpy (num_videos x max_frame_audio_sequence_length x 128)\n",
    "    for frame in frame_audio:\n",
    "        # stack the frames in each video, only allowed number of first frams\n",
    "        #print(np.vstack(frame).shape)\n",
    "        frames_audio.append(np.vstack(frame)[:max_frame_audio_sequence_length,:])\n",
    "\n",
    "    frames_audio = np.reshape(np.array(frames_audio),(len(frame_audio),max_frame_audio_sequence_length,128))\n",
    "    #print(frames_audio.shape)\n",
    "    \n",
    "    # deal with videos\n",
    "    \n",
    "    video_rgb = np.vstack(video_rgb)\n",
    "    video_audio = np.vstack(video_audio)\n",
    "    \n",
    "    \n",
    "    # labels - need to one-hot encode TOP - K label\n",
    "    labels = one_hot_y(labels,label_feature_size)\n",
    "    labels = np.vstack(labels)\n",
    "    return frames,frames_audio, video_rgb,video_audio, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run on SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pandas(frame_rgb, frame_audio, video_rgb, video_audio, labels):\n",
    "    '''\n",
    "    Spark unfortunately does not work with numpy arrays - so we need to convert to traditional python types.\n",
    "    '''\n",
    "    return pd.DataFrame.from_dict({'frame_rgb':[[[float(k) for k in j] for j in i] for i in frame_rgb],\\\n",
    "                             'frame_audio':[[[float(k) for k in j] for j in i] for i in frame_audio],\\\n",
    "                             'mean_rgb':[[float(j) for j in i] for i in list(video_rgb)],\\\n",
    "                            'mean_audio':[[float(j) for j in i] for i in list(video_audio)],\\\n",
    "                            'labels':[[float(j) for j in i] for i in list(labels)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('Youtube-8M') \\\n",
    "                  .set(\"spark.jars\",\n",
    "                       \"ecosystem/spark/spark-tensorflow-connector/target/spark-tensorflow-connector_2.11-1.10.0.jar\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-adf8daf949ef>:23: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:04<00:00,  9.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# We just load all of the video data in memory since it is fairly small and manageable.\n",
    "train_vid_ids, train_labels, train_mean_rgb, train_mean_audio \\\n",
    "    = extract_video_files(\"mys3bucket/yt8pm_100th_shard/v2/video/train*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fun starts here, pull frame data per tf-record\n",
    "train_frame_shards = glob('mys3bucket/yt8pm_100th_shard/v2/frame/train*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1015 videos in this TF record.\n",
      "iterating video: 9 ,frames: 122/123"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:33<00:33, 33.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 1029 videos in this TF record.\n",
      "iterating video: 9 ,frames: 127/128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:03<00:00, 32.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# iterate tf records one by one and append to spark dataframe\n",
    "\n",
    "FIRST_RECORD = True # flag whether to append to Spark SQL or create a new one\n",
    "NUM_RECORDS_TO_LOAD = 1\n",
    "\n",
    "for tf_record in tqdm(train_frame_shards[:NUM_RECORDS_TO_LOAD]):\n",
    "    # pull frames in memory\n",
    "    train_frame_ids, train_frame_labels,train_frame_rgb,train_frame_audio \\\n",
    "        = extract_frame_level_features_per_tf_record(tf_record,maximum_iter=True,\\\n",
    "                               stop_at_iter=10) # just pull 10 videos from each tf record for debugging\n",
    "    # first transformation\n",
    "    train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio, \\\n",
    "    train_labels, val_video_rgb, val_video_audio, val_frame_rgb, val_frame_audio, val_labels \\\n",
    "                = create_train_dev_dataset(train_mean_rgb, train_mean_audio, train_vid_ids, train_frame_rgb, \\\n",
    "                                            train_frame_audio, train_frame_labels, train_frame_ids )    \n",
    "    \n",
    "    # final transformation for LSTM\n",
    "    train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio, train_labels = \\\n",
    "            transform_data_for_lstm(train_video_rgb, train_video_audio, train_frame_rgb, train_frame_audio,train_labels)\n",
    "    \n",
    "    val_frame_rgb, val_frame_audio, val_video_rgb, val_video_audio, val_labels = \\\n",
    "            transform_data_for_lstm( val_video_rgb, val_video_audio,val_frame_rgb, val_frame_audio, val_labels)\n",
    "    \n",
    "    #### BELOW WE ONLY USE THE TRAINING DATA AND NO VALIDATION DATA FOR SIMPLICITY\n",
    "    df = create_pandas(train_frame_rgb, train_frame_audio, \\\n",
    "                   train_video_rgb, train_video_audio, train_labels)\n",
    "    # create spark data frame\n",
    "    if FIRST_RECORD:\n",
    "        df_spark = spark.createDataFrame(df)\n",
    "        FIRST_RECORD = False\n",
    "    else:\n",
    "        df_spark_new = spark.createDataFrame(df)\n",
    "        df_spark = df_spark.union(df_spark_new)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DYLAN - YOUR JOB IS TO FIGURE OUT HOW TO RUN THE ELEPHAS TRAINING NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|         frame_audio|           frame_rgb|              labels|          mean_audio|            mean_rgb|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[WrappedArray(101...|[WrappedArray(53....|[1.0, 0.0, 0.0, 0...|[-1.0705330371856...|[-1.1001880168914...|\n",
      "|[WrappedArray(173...|[WrappedArray(170...|[0.0, 1.0, 0.0, 0...|[1.12153804302215...|[-0.0249195266515...|\n",
      "|[WrappedArray(173...|[WrappedArray(0.0...|[0.0, 1.0, 0.0, 0...|[-0.2137299776077...|[-0.3027234375476...|\n",
      "|[WrappedArray(155...|[WrappedArray(105...|[1.0, 0.0, 0.0, 0...|[-0.9770614504814...|[-0.5987871885299...|\n",
      "|[WrappedArray(183...|[WrappedArray(209...|[0.0, 0.0, 0.0, 0...|[0.93089854717254...|[0.51462107896804...|\n",
      "|[WrappedArray(192...|[WrappedArray(127...|[0.0, 0.0, 1.0, 1...|[1.11850774288177...|[-0.3764120936393...|\n",
      "|[WrappedArray(103...|[WrappedArray(97....|[0.0, 0.0, 0.0, 0...|[-1.1013855934143...|[0.30566042661666...|\n",
      "|[WrappedArray(173...|[WrappedArray(33....|[0.0, 0.0, 0.0, 0...|[-0.8527888059616...|[0.19254451990127...|\n",
      "|[WrappedArray(131...|[WrappedArray(137...|[0.0, 0.0, 0.0, 0...|[-0.3261844813823...|[-0.2825042307376...|\n",
      "|[WrappedArray(143...|[WrappedArray(149...|[1.0, 0.0, 0.0, 0...|[-0.1917169094085...|[0.31557720899581...|\n",
      "|[WrappedArray(66....|[WrappedArray(157...|[1.0, 1.0, 0.0, 0...|[-1.4782097339630...|[0.63310778141021...|\n",
      "|[WrappedArray(114...|[WrappedArray(164...|[0.0, 0.0, 0.0, 0...|[0.22108773887157...|[0.48961350321769...|\n",
      "|[WrappedArray(173...|[WrappedArray(0.0...|[0.0, 0.0, 0.0, 1...|[1.34446597099304...|[-0.1465962529182...|\n",
      "|[WrappedArray(145...|[WrappedArray(172...|[0.0, 0.0, 0.0, 0...|[-0.5627757310867...|[0.71467524766922...|\n",
      "|[WrappedArray(113...|[WrappedArray(0.0...|[0.0, 0.0, 1.0, 0...|[1.04404509067535...|[-0.7358107566833...|\n",
      "|[WrappedArray(163...|[WrappedArray(0.0...|[0.0, 0.0, 0.0, 1...|[1.15008413791656...|[-0.7002602815628...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = df_spark.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the order of vars to fit the order in the model!\n",
    "# train_frame_rgb, train_frame_audio, train_video_rgb, train_video_audio], train_labels\n",
    "train_rdd = train_rdd.map(lambda x: (np.array(x[1]), np.array(x[0]),np.array(x[4]),np.array(x[3]),np.array(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10, 128)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200)          900000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 200)          183200      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 400)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          512500      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 400)          1600        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 500)          2000        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 500)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          200500      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 500)          250500      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000)         0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1000)         4000        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1000)         1001000     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1000)         4000        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           10010       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,069,310\n",
      "Trainable params: 3,063,510\n",
      "Non-trainable params: 5,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "keras_model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model = SparkModel(keras_model, frequency='batch', mode='synchronous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 20, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in train\n    x_train = np.asarray([x for x, y in feature_iterator])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in <listcomp>\n    x_train = np.asarray([x for x, y in feature_iterator])\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in train\n    x_train = np.asarray([x for x, y in feature_iterator])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in <listcomp>\n    x_train = np.asarray([x for x, y in feature_iterator])\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-27dfe73abae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, rdd, epochs, batch_size, verbose, validation_split)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'asynchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'synchronous'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hogwild'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/spark_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, rdd, epochs, batch_size, verbose, validation_split)\u001b[0m\n\u001b[1;32m    186\u001b[0m             worker = SparkWorker(yaml, parameters, train_config,\n\u001b[1;32m    187\u001b[0m                                  optimizer, loss, metrics, custom)\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mnew_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_master_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# simply accumulate gradients one by one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 20, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in train\n    x_train = np.asarray([x for x, y in feature_iterator])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in <listcomp>\n    x_train = np.asarray([x for x, y in feature_iterator])\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in train\n    x_train = np.asarray([x for x, y in feature_iterator])\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/elephas/worker.py\", line 36, in <listcomp>\n    x_train = np.asarray([x for x, y in feature_iterator])\nValueError: too many values to unpack (expected 2)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# HOW TO MAKE SURE THIS WORKS???\n",
    "history = spark_model.fit(train_rdd, epochs=10, batch_size=2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
